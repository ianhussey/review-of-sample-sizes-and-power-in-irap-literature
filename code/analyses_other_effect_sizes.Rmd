---
title: "N-Pact Factor assessment for IRAP research"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)

```

# Dependencies

```{r}

library(pwr)
library(tidyverse)
library(janitor)
library(viridis)
library(knitr)
library(kableExtra)
library(bayestestR)
library(scales)
library(sjPlot)
library(performance)

# create directory for plots
dir.create("plots")

```

# Data

```{r}

data_combined <- read_csv("../data/processed/data_fraley_and_irap.csv") |>
  # clean names and values
  rename(N = n_participants_after_exclusions,
         design = study_design_ignoring_trial_type_comparisons) |>
  mutate(year = str_replace(publication_year, "20", "'"),
         field = ifelse(journal == "IRAP", "IRAP research", "Social Psychology"),
         design = case_when(design == "b" ~ "between",
                            design == "w" ~ "within",
                            design == "m" ~ "mixed"),
         reported_n = !is.na(N))

```

# Descriptives

## IRAP studies

```{r}

# subset data
data_irap_n <- data_combined |>
  filter(journal == "IRAP") 

# calculate k articles
k_articles <- data_irap_n |>
  distinct(key) |>
  count() |>
  pull(n)

# calculate k studies
k_studies <- data_irap_n |>
  count()  |>
  pull(n)

# table
data_irap_n |>         
  summarize(`Total articles` = k_articles,
            `Total studies` = k_studies) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

## Social and Personality Psychology studies

```{r}

data_combined_comparison <- data_combined |>
  filter(
    # filters for social and personality psych studies 
    # filter out non-social studies published in psych science
    (design == "between" & ((journal == "PS:S" & (social_ps == 1 | is.na(social_ps))) | journal != "PS:S")) |
      # filters for irap studies
      (journal == "IRAP" & design %in% c("between", "mixed") & used_inferential_statistics == TRUE) 
  ) |>
  filter(reported_n == TRUE)
  

data_combined_comparison |>
  filter(field != "IRAP research") |>
  count() |>
  kable() |>
  kable_classic(full_width = FALSE)

data_combined_comparison |>
  filter(field != "IRAP research") |>
  count(journal) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# Prevalence of NHST in the IRAP literature

Using the full sample

```{r}

# table by use of inferential stats
data_irap_n |>   
  count(`Reported N` = reported_n, 
        `Studies using inferential statistics` = used_inferential_statistics) |>
  mutate(percent = janitor::round_half_up(n/sum(n)*100, 1)) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# IRAP N per sample

All analyses from here are based on IRAP publications that employed NHST. 

## Descriptives

```{r}

# subset data. reused again further below
data_combined_using_nhst <- data_combined |>
  filter(used_inferential_statistics == TRUE & 
           reported_n == TRUE)

# subset further
data_irap_n_using_nhst <- data_combined_using_nhst |>
  filter(journal == "IRAP") 

# count articles
k_articles <- data_irap_n_using_nhst |>
  distinct(key) |>
  count() |>
  pull(n)

# count studies
k_studies <- data_irap_n_using_nhst |>
  count()  |>
  pull(n)

# calculate maximum a posteriori estimate (mode of a continuous variable) 
MAP <- bayestestR::map_estimate(data_irap_n_using_nhst$N) |>
  janitor::round_half_up(1) |>
  as.numeric()

# combine table
data_irap_n_using_nhst |>         
  summarize(`Total articles` = k_articles,
            `Total studies` = k_studies,
            `Total N` = sum(N),
            Median = median(N),
            MAD = janitor::round_half_up(mad(N), 1),
            Min = min(N),
            Max = max(N)) |>
  mutate(MAP = MAP) |>
  select(`Total articles`,
         `Total studies`,
         `Total N`,
         `MAP N` = MAP,
         `Median N` = Median,
         `MAD N` = MAD,
         `Min N` = Min,
         `Max N` = Max) |>
  kable() |>
  kable_classic(full_width = FALSE)

# plot
p_distribution <- 
  ggplot(data_irap_n_using_nhst, aes(N)) +
  geom_histogram(binwidth = 10) +
  theme_classic() +
  ylab("Count of studies") +
  xlab("Sample size (N)") +
  scale_x_continuous(breaks = seq(from = 0, to = 250, by = 10))

p_distribution

ggsave("plots/p_distribution.pdf",
       plot = p_distribution,
       width = 6,
       height = 4,
       units = "in")

```

## Split by study design

Note that IRAP publications with mixed designs also report results from between groups analyses, and so both mixed and between are included in the power analyses further sections below.

```{r}

data_combined_using_nhst |>
  distinct(title, design) |>
  count(design) |>
  mutate(percent = janitor::round_half_up(n/sum(n)*100, 1)) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

## Change over time

### Table

```{r}

# subset irap data 
data_table_1_filtered <- data_combined_using_nhst |>
  filter(field == "IRAP research")

# calculate median N for each field and year
# round using typical half-up rule and not R's weird rounding
data_table_1_n_long <- data_table_1_filtered |>
  group_by(field, year) |>
  summarize(median_n = janitor::round_half_up(median(N, na.rm = TRUE), 0)) |>
  ungroup()

# Calculate median median n for each field
data_table_1_n_aggregate <- data_table_1_n_long |>
  group_by(field) |>
  summarize(median_median_n = median(median_n, na.rm = TRUE)) |>
  ungroup()

# Calculate k studies used to calculate median ns
data_table_1_k_long <- data_table_1_filtered |>
  count(field, year, name = "k_studies")

# combine Ns and Ks, reshape, and order field column by aggregate median median n 
data_table_1 <- 
  full_join(data_table_1_n_long, data_table_1_k_long, by = c("field", "year")) |>
  mutate(result = paste0(median_n, " (", k_studies, ")")) |>
  select(-median_n, -k_studies) |>
  pivot_wider(names_from = year, 
              values_from = result) |>
              #names_prefix = "NF ") |>
  left_join(data_table_1_n_aggregate, by = "field") |>
  mutate(field = fct_reorder(field, median_median_n)) |>
  arrange(desc(field)) |>
  select(field, median_median_n, 
         `'06`, `'07`, `'08`, `'09`, `'10`, `'11`, 
         `'12`, `'13`, `'14`, `'15`, `'16`, `'17`, 
         `'18`, `'19`, `'20`, `'21`, `'22`)

# print table
data_table_1 |>
  select(field, 
         Aggregate = median_median_n, 
         starts_with("'")) |>
         #starts_with("NF")) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

### Figure

```{r}

# Filter out non-social studies published in psych science
# calculate median Ns
# order field column by same used in Table 1
data_figure_1 <- data_table_1_filtered |>
  group_by(field, year) |>
  summarize(median_n = janitor::round_half_up(median(N, na.rm = TRUE), 0)) |>
  ungroup() |>
  mutate(field = fct_rev(fct_relevel(field, levels(data_table_1$field))))

# plot 
ggplot(data_figure_1, aes(as.factor(year), median_n, group = field)) +
  geom_smooth(method = "lm", alpha = 0.2) +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "plasma", begin = 0.1, end = 0.9) +
  theme_classic() +
  ylab("Median sample size (N) per study") +
  xlab("Year") 

```

### Estimate change in median N over time

```{r fig.height=10, fig.width=6}

data_for_model_1 <- data_figure_1 |>
  mutate(year = as.numeric(str_remove(year, "'")),
         year_recentered = year - 06)

fit_1 <- lm(formula = median_n ~ year_recentered,
                data = data_for_model_1)

#check_model(fit_1)
#check_normality(fit_1)

tab_model(fit_1, 
          string.est = "B",
          digits = 1,
          ci.hyphen = ", ")

```

# IRAP N per group

## Median sample size 

### Table

```{r}

# Filter out non-social studies published in psych science
data_table_2_filtered <- data_combined_using_nhst |>
  filter(journal == "IRAP" & design %in% c("between", "mixed")) |>
  mutate(N_per_cell = N/n_groups_between)

# calculate median N for each field and year
# round using typical half-up rule and not R's weird rounding
data_table_2_n_long <- data_table_2_filtered |>
  group_by(field, year) |>
  summarize(median_n_per_cell = janitor::round_half_up(median(N_per_cell, na.rm = TRUE), 0)) |>
  ungroup()

# Calculate median median n for each field
data_table_2_n_aggregate <- data_table_2_n_long |>
  group_by(field) |>
  summarize(median_median_n_per_cell = median(median_n_per_cell, na.rm = TRUE)) |>
  ungroup()

# Calculate k studies used to calculate median ns
data_table_2_k_long <- data_table_2_filtered |>
  count(field, year, name = "k_studies")

# combine Ns and Ks, reshape, and order field column by aggregate median median n 
data_table_2 <- 
  full_join(data_table_2_n_long, data_table_2_k_long, by = c("field", "year")) |>
  mutate(result = paste0(median_n_per_cell, " (", k_studies, ")")) |>
  select(-median_n_per_cell, -k_studies) |>
  pivot_wider(names_from = year, 
              values_from = result) |>
              #names_prefix = "NF ") |>
  left_join(data_table_2_n_aggregate, by = "field") |>
  mutate(field = fct_reorder(field, median_median_n_per_cell)) |>
  arrange(desc(field)) |>
  select(field, median_median_n_per_cell, 
         `'06`, `'07`, `'08`, `'09`, `'10`, `'11`, 
         `'12`, `'13`, `'14`, `'15`, `'16`, `'17`, 
         `'18`, `'19`, `'20`, `'21`, `'22`)

# print table
data_table_2 |>
  select(field, 
         Aggregate = median_median_n_per_cell, 
         starts_with("'")) |>
         #starts_with("NF")) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

### Figure

```{r}

# Filter out non-social studies published in psych science
# calculate median Ns
# order field column by same used in Table 1
data_figure_2 <- data_table_2_filtered |>
  group_by(field, year) |>
  summarize(median_n_per_cell = janitor::round_half_up(median(N_per_cell, na.rm = TRUE), 0)) |>
  ungroup() |>
  mutate(field = fct_rev(fct_relevel(field, levels(data_table_1$field))))

# plot 
ggplot(data_figure_2, aes(as.factor(year), median_n_per_cell, group = field)) +
  geom_smooth(method = "lm") +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "plasma", begin = 0.1, end = 0.9) +
  theme_classic() +
  ylab("Median sample size (N) per cell") +
  xlab("Year") 

```

### Estimate change in median N over time

```{r fig.height=10, fig.width=6}

data_for_model_2 <- data_figure_2 |>
  mutate(year = as.numeric(str_remove(year, "'")),
         year_recentered = year - 06)

fit_2 <- lm(formula = median_n_per_cell ~ year_recentered,
                data = data_for_model_2)

#check_model(fit_2)
#check_normality(fit_2)

tab_model(fit_2, 
          string.est = "B",
          digits = 1,
          ci.hyphen = ", ")

```

Note that data was re-centered so that the intercept is 2006. 

Predictable violations of assumptions due to the skew in sample sizes mean that results must be interpreted in much caution. No p values are presented as no hypotheses were tested: the purpose of the regression was to provide an illustrative estimate of the change in sample size per year.

#### Combined plot

Probably most useful for publication

```{r}

p_median_n_over_time <- 
  # combine data
  bind_rows(
    data_figure_1 |>
      mutate(type = "Participants per study (all studies)"),
    data_figure_2 |>
      rename(median_n = median_n_per_cell) |>
      mutate(type = "Participants per group (studies with between-subjects comparisons)")
  ) |>
  # plot
  ggplot(aes(as.factor(year), median_n, group = type, color = type)) +
  geom_smooth(method = "lm", alpha = 0.2) +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "viridis", begin = 0.3, end = 0.7,
                        guide = guide_legend(reverse = TRUE)) +
  theme_classic() +
  ylab("Median sample size (N)") +
  xlab("Year") +
  theme(legend.position = "bottom",
        legend.direction = "vertical",
        legend.title = element_blank())

p_median_n_over_time

ggsave("plots/p_median_n_over_time.pdf",
       plot = p_median_n_over_time,
       width = 6,
       height = 5,
       units = "in")

```

## Implied Statistical Power 

### $\rho$ = .20

#### Table

```{r}

# use median n to calculate power
data_table_3_long <- data_table_2_n_long |>
  #mutate(power = janitor::round_half_up(pwr.r.test(r = .2, sig.level = 0.05, n = median_n_per_cell)$power*100, 0)) |>
  mutate(power = janitor::round_half_up(pwr.t.test(d = .408, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0)) |>
  select(-median_n_per_cell)

# reshape table, order field column by that used in previous table
data_table_3 <- data_table_3_long |>
  pivot_wider(names_from = year, 
              values_from = power) |>
              #names_prefix = "power ") |>
  mutate(field = fct_relevel(field, levels(data_table_1$field))) |>
  arrange(desc(field)) |>
  select(field, 
         `'06`, `'07`, `'08`, `'09`, `'10`, `'11`, 
         `'12`, `'13`, `'14`, `'15`, `'16`, `'17`, 
         `'18`, `'19`, `'20`, `'21`, `'22`)

# print table
data_table_3 |>
  kable() |>
  kable_classic(full_width = FALSE)

```

#### Figure

```{r}

# order field column by same used in Table 1, then plot
p_power <- data_table_3_long |>
  mutate(field = fct_rev(fct_relevel(field, levels(data_table_1$field)))) |>
  ggplot(aes(as.factor(year), power/100, group = field)) +
  geom_hline(yintercept = .80, linetype = "dotted") +
  geom_smooth(method = "lm", alpha = 0.2, colour = "black") +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "plasma", begin = 0.1, end = 0.9) +
  scale_y_continuous(labels = scales::label_comma(), 
                     breaks = c(0, .2, .4, .6, .8, 1), 
                     limits = c(0,1)) +
  theme_classic() +
  ylab("Power") +
  xlab("Year")

p_power

ggsave("plots/p_power.pdf",
       plot = p_power,
       width = 6,
       height = 4,
       units = "in")

```

#### Estimate change  in power over time

```{r fig.height=10, fig.width=6}

data_for_model_3 <- data_table_3_long |>
  mutate(year = as.numeric(str_remove(year, "'")),
         year_recentered = year - 6,
         power = power/100)

fit_3 <- lm(formula = power ~ year_recentered,
            data = data_for_model_3)

#check_model(fit_3)
#check_normality(fit_3)

tab_model(fit_3, 
          string.est = "B",
          digits = 3,
          ci.hyphen = ", ")

```

Note that data was re-centered so that the intercept is 2006. 

```{r}

predicted_power_2022 <- (0.142 + 0.009*(2022-2006)) # values taken from regression results table

necessary_increase_to_reach_power_.80 <- .80 - predicted_power_2022

years_until_power_.80 <- ceiling(necessary_increase_to_reach_power_.80 / 0.009)

```

If this rate of increase in median sample sizes is maintained, power of at least .80 (Cohen, 1988) in the median IRAP study would be achieved `r years_until_power_.80` years from now, i.e., by `r 2022 + years_until_power_.80`.


### ADDITION: $\rho$ = .45 (Vahey et al. 2015) = Cohen's d = 1.008

I extend on the range of effect sizes considered by Fraley et al. (2022) to also consider Cohen's (1988) recommendations for small, medium, and larger Cohen's d effect sizes. 

```{r fig.height=5, fig.width=6}


# use median n to calculate power
data_table_3_long_alt <- data_table_2_n_long |>
  #mutate(power = janitor::round_half_up(pwr.r.test(r = .2, sig.level = 0.05, n = median_n_per_cell)$power*100, 0)) |>
  mutate(power_for_cohens_d_.408 = janitor::round_half_up(pwr.t.test(d = 0.408, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0),
         power_for_cohens_d_1.01 = janitor::round_half_up(pwr.t.test(d = 1.008, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0)) |>
  select(-median_n_per_cell)

# # print table
# data_table_3_long_alt |>
#   kable() |>
#   kable_classic(full_width = FALSE)


# order field column by same used in Table 1, then plot
p_power_alt <- data_table_3_long_alt |>
  pivot_longer(cols = c("power_for_cohens_d_.408", "power_for_cohens_d_1.01"),
               names_to = "effect_size",
               values_to = "power") |>
  mutate(effect_size = as.factor(str_replace(effect_size, "power_for_cohens_d_", "Cohen's d = "))) |>
  ggplot(aes(as.factor(year), power/100, group = effect_size, color = effect_size)) +
  geom_hline(yintercept = .80, linetype = "dotted") +
  geom_smooth(method = "lm", alpha = 0.2) +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "viridis", begin = 0.3, end = 0.7,
                        guide = guide_legend(reverse = TRUE),
                        labels = c("Pearson's r = .45, Cohen's d = 1.008", 
                                   "Pearson's r = .20, Cohen's d = 0.408")) +
  scale_y_continuous(labels = scales::label_comma(), 
                     breaks = c(0, .2, .4, .6, .8, 1)) +
  theme_classic() +
  ylab("Power") +
  xlab("Year")  +
  labs(colour = "Effect size") +
  theme(legend.position = "bottom",
        legend.direction = "vertical") +
  coord_cartesian(ylim = c(0,1))

p_power_alt

# ggsave("plots/p_power_alt.pdf",
#        plot = p_power_alt,
#        width = 6,
#        height = 5,
#        units = "in")

```

- This entirely rides on one's faith in Vahey et al's estimate, which I know to be both impossibly large and non reproducible.

### ADDITION: Cohen's d = .2, .5, .8 attenuated for iraps reliability

take the most liberal estimate of the IRAP's reliability = .60 (greenwalkd and lai 2020) and assume the other measures have an average of perfect reliability = .90 (excellent).

used https://www.escal.site/ to convert d = .2, .5, .8 to r = .1, .243, .371 

correlation attenuation formula is rho_xy = r_xy * sqrt(r_xx * r_xy) 

observable small effect = .1 * sqrt(0.9 * .60) = 0.073
converted back to d = 0.146

observable medium effect = .243 * sqrt(0.9 * .60) = 0.179
converted back to d = 0.364

observable large effect = .371 * sqrt(0.9 * .60) = 0.273
converted back to d = 0.568

```{r fig.height=5, fig.width=6}

# use median n to calculate power
data_table_3_long_alt <- data_table_2_n_long |>
  #mutate(power = janitor::round_half_up(pwr.r.test(r = .2, sig.level = 0.05, n = median_n_per_cell)$power*100, 0)) |>
  mutate(power_for_small = janitor::round_half_up(pwr.t.test(d = 0.146, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0),
         power_for_medium = janitor::round_half_up(pwr.t.test(d = 0.364, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0),
         power_for_large = janitor::round_half_up(pwr.t.test(d = 0.568, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0)) |>
  select(-median_n_per_cell)

# # print table
# data_table_3_long_alt |>
#   kable() |>
#   kable_classic(full_width = FALSE)

# order field column by same used in Table 1, then plot
p_power_alt <- data_table_3_long_alt |>
  pivot_longer(cols = c("power_for_small", "power_for_medium", "power_for_large"),
               names_to = "effect_size",
               values_to = "power") |>
  ggplot(aes(as.factor(year), power/100, group = effect_size, color = effect_size)) +
  geom_hline(yintercept = .80, linetype = "dotted") +
  geom_smooth(method = "lm", alpha = 0.2) +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "viridis", begin = 0.3, end = 0.7,
                        guide = guide_legend(reverse = TRUE)) +
  scale_y_continuous(labels = scales::label_comma(), 
                     breaks = c(0, .2, .4, .6, .8, 1)) +
  theme_classic() +
  ylab("Power") +
  xlab("Year")  +
  labs(colour = "Effect size") +
  theme(legend.position = "bottom",
        legend.direction = "vertical") +
  coord_cartesian(ylim = c(0,1))

p_power_alt

# ggsave("plots/p_power_alt.pdf",
#        plot = p_power_alt,
#        width = 6,
#        height = 5,
#        units = "in")

```


### ADDITION: Cohen's d = .2, .5, .8 attenuated for iraps reliability

choose less optimistic estimate of IRAP's reliability = .51 (Hussey and Drake, overall d score)
reliability of other measures excellent but not perfect = .90

used https://www.escal.site/ to convert d = .2, .5, .8 to r = .1, .243, .371 

correlation attenuation formula is rho_xy = r_xy * sqrt(r_xx * r_xy) 

observable small effect = .1 * sqrt(0.9 * .51) = 0.068
converted back to d = 0.136

observable medium effect = .243 * sqrt(0.9 * .51) = 0.165
converted back to d = 0.335

observable large effect = .371 * sqrt(0.9 * .51) = 0.251
converted back to d = 0.519

```{r fig.height=5, fig.width=6}

# use median n to calculate power
data_table_3_long_alt2 <- data_table_2_n_long |>
  #mutate(power = janitor::round_half_up(pwr.r.test(r = .2, sig.level = 0.05, n = median_n_per_cell)$power*100, 0)) |>
  mutate(power_for_small = janitor::round_half_up(pwr.t.test(d = 0.136, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0),
         power_for_medium = janitor::round_half_up(pwr.t.test(d = 0.335, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0),
         power_for_large = janitor::round_half_up(pwr.t.test(d = 0.519, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0)) |>
  select(-median_n_per_cell)

# # print table
# data_table_3_long_alt |>
#   kable() |>
#   kable_classic(full_width = FALSE)

# order field column by same used in Table 1, then plot
p_power_alt2 <- data_table_3_long_alt2 |>
  pivot_longer(cols = c("power_for_small", "power_for_medium", "power_for_large"),
               names_to = "effect_size",
               values_to = "power") |>
  ggplot(aes(as.factor(year), power/100, group = effect_size, color = effect_size)) +
  geom_hline(yintercept = .80, linetype = "dotted") +
  geom_smooth(method = "lm", alpha = 0.2) +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "viridis", begin = 0.3, end = 0.7,
                        guide = guide_legend(reverse = TRUE)) +
  scale_y_continuous(labels = scales::label_comma(), 
                     breaks = c(0, .2, .4, .6, .8, 1)) +
  theme_classic() +
  ylab("Power") +
  xlab("Year")  +
  labs(colour = "Effect size") +
  theme(legend.position = "bottom",
        legend.direction = "vertical") +
  coord_cartesian(ylim = c(0,1))

p_power_alt2

# ggsave("plots/p_power_alt.pdf",
#        plot = p_power_alt,
#        width = 6,
#        height = 5,
#        units = "in")

```

### ADDITION: Cohen's d = .2, .5, .8 attenuated for iraps reliability

choose less optimistic estimate of IRAP's reliability = .28 (Hussey and Drake, by trial type)
reliability of other measures excellent but not perfect = .90

used https://www.escal.site/ to convert d = .2, .5, .8 to r = .1, .243, .371 

correlation attenuation formula is rho_xy = r_xy * sqrt(r_xx * r_xy) 

observable small effect = .1 * sqrt(0.9 * .28) = 0.050
converted back to d = 0.100

observable medium effect = .243 * sqrt(0.9 * .28) = 0.122
converted back to d = 0.246

observable large effect = .371 * sqrt(0.9 * .28) = 0.186
converted back to d = 0.379

```{r fig.height=5, fig.width=6}

# use median n to calculate power
data_table_3_long_alt2 <- data_table_2_n_long |>
  #mutate(power = janitor::round_half_up(pwr.r.test(r = .2, sig.level = 0.05, n = median_n_per_cell)$power*100, 0)) |>
  mutate(power_for_small = janitor::round_half_up(pwr.t.test(d = 0.100, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0),
         power_for_medium = janitor::round_half_up(pwr.t.test(d = 0.246, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0),
         power_for_large = janitor::round_half_up(pwr.t.test(d = 0.379, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0)) |>
  select(-median_n_per_cell)

# # print table
# data_table_3_long_alt |>
#   kable() |>
#   kable_classic(full_width = FALSE)

# order field column by same used in Table 1, then plot
p_power_alt2 <- data_table_3_long_alt2 |>
  pivot_longer(cols = c("power_for_small", "power_for_medium", "power_for_large"),
               names_to = "effect_size",
               values_to = "power") |>
  ggplot(aes(as.factor(year), power/100, group = effect_size, color = effect_size)) +
  geom_hline(yintercept = .80, linetype = "dotted") +
  geom_smooth(method = "lm", alpha = 0.2) +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "viridis", begin = 0.3, end = 0.7,
                        guide = guide_legend(reverse = TRUE)) +
  scale_y_continuous(labels = scales::label_comma(), 
                     breaks = c(0, .2, .4, .6, .8, 1)) +
  theme_classic() +
  ylab("Power") +
  xlab("Year")  +
  labs(colour = "Effect size") +
  theme(legend.position = "bottom",
        legend.direction = "vertical") +
  coord_cartesian(ylim = c(0,1))

p_power_alt2

# ggsave("plots/p_power_alt.pdf",
#        plot = p_power_alt,
#        width = 6,
#        height = 5,
#        units = "in")

```



