---
title: "N-Pact Factor assessment for IRAP research"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# Notes of differences between original Fraley et al. analysis and my reanalysis 

Limited to between-only designs. NB original Fraley et al. (2022) study used only between-groups studies in the tables but plotted all studies. Here I use all between groups social psych studies and both between and mixed IRAP studies, as all mixed IRAP studies also reported between analyses.

```{r setup, include=FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)

```

# Dependencies

```{r}

library(pwr)
library(tidyverse)
library(janitor)
library(viridis)
library(knitr)
library(kableExtra)
library(bayestestR)
library(scales)

```

# Data

```{r}

data_combined <- read_csv("../data/processed/data_fraley_and_irap.csv") |>
  rename(N = n_participants_after_exclusions,
         design = study_design_ignoring_trial_type_comparisons) |>
  mutate(year = str_replace(publication_year, "20", "'"),
         field = ifelse(journal == "IRAP", "IRAP research", "Social Psychology"),
         design = case_when(design == "b" ~ "between",
                            design == "w" ~ "within",
                            design == "m" ~ "mixed"),
         reported_n = !is.na(N))

```

# Prevalence of NHST in the IRAP literature

Using the full sample

```{r}

# subset data
data_irap_n <- data_combined |>
  filter(journal == "IRAP") 

# calculate k articles
k_articles <- data_irap_n |>
  distinct(key) |>
  count() |>
  pull(n)

# calculate k studies
k_studies <- data_irap_n |>
  count()  |>
  pull(n)

# table
data_irap_n |>         
  summarize(`Total articles` = k_articles,
            `Total studies` = k_studies) |>
  kable() |>
  kable_classic(full_width = FALSE)

# table by use of inferential stats
data_irap_n |>   
  count(`Reported N` = reported_n, 
        `Studies using inferential statistics` = used_inferential_statistics) |>
  mutate(percent = janitor::round_half_up(n/sum(n)*100, 1)) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# IRAP studies sample size

All analyses from here are based on IRAP publications that employed NHST. 

## Descriptives

```{r}

# subset data. reused again further below
data_combined_using_nhst <- data_combined |>
  filter(used_inferential_statistics == TRUE & 
           reported_n == TRUE)

# subset further
data_irap_n_using_nhst <- data_combined_using_nhst |>
  filter(journal == "IRAP") 

# count articles
k_articles <- data_irap_n_using_nhst |>
  distinct(key) |>
  count() |>
  pull(n)

# count studies
k_studies <- data_irap_n_using_nhst |>
  count()  |>
  pull(n)

# calculate maximum a posteriori estimate (mode of a continuous variable) 
MAP <- bayestestR::map_estimate(data_irap_n_using_nhst$N) |>
  janitor::round_half_up(1) |>
  as.numeric()

# combine table
data_irap_n_using_nhst |>         
  summarize(`Total articles` = k_articles,
            `Total studies` = k_studies,
            `Total N` = sum(N),
            Median = median(N),
            MAD = janitor::round_half_up(mad(N), 1),
            Min = min(N),
            Max = max(N)) |>
  mutate(MAP = MAP) |>
  select(`Total articles`,
         `Total studies`,
         `Total N`,
         `MAP N` = MAP,
         `Median N` = Median,
         `MAD N` = MAD,
         `Min N` = Min,
         `Max N` = Max) |>
  kable() |>
  kable_classic(full_width = FALSE)

# plot
ggplot(data_irap_n_using_nhst, aes(N)) +
  geom_histogram(binwidth = 10) +
  theme_classic() +
  ylab("Number of studies") +
  xlab("Sample size after exclusions (N)") +
  scale_x_continuous(breaks = seq(from = 0, to = 250, by = 10))

```

## Split by study design

Note that IRAP publications with mixed designs also report results from between groups analyses, and so both mixed and between are included in the power analyses further sections below.

```{r}

data_combined_using_nhst |>
  filter(!is.na(N) & journal == "IRAP") |>
  distinct(title, design) |>
  count(design) |>
  mutate(percent = janitor::round_half_up(n/sum(n), 2)) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

## Change over time

### Table

```{r}

# subset irap data 
data_table_1_filtered <- data_combined_using_nhst |>
  filter(field == "IRAP research")

# calculate median N for each field and year
# round using typical half-up rule and not R's weird rounding
data_table_1_n_long <- data_table_1_filtered |>
  group_by(field, year) |>
  summarize(median_n = janitor::round_half_up(median(N, na.rm = TRUE), 0)) |>
  ungroup()

# Calculate median median n for each field
data_table_1_n_aggregate <- data_table_1_n_long |>
  group_by(field) |>
  summarize(median_median_n = median(median_n, na.rm = TRUE)) |>
  ungroup()

# Calculate k studies used to calculate median ns
data_table_1_k_long <- data_table_1_filtered |>
  count(field, year, name = "k_studies")

# combine Ns and Ks, reshape, and order field column by aggregate median median n 
data_table_1 <- 
  full_join(data_table_1_n_long, data_table_1_k_long, by = c("field", "year")) |>
  mutate(result = paste0(median_n, " (", k_studies, ")")) |>
  select(-median_n, -k_studies) |>
  pivot_wider(names_from = year, 
              values_from = result) |>
              #names_prefix = "NF ") |>
  left_join(data_table_1_n_aggregate, by = "field") |>
  mutate(field = fct_reorder(field, median_median_n)) |>
  arrange(desc(field)) |>
  select(field, median_median_n, 
         `'06`, `'07`, `'08`, `'09`, `'10`, `'11`, 
         `'12`, `'13`, `'14`, `'15`, `'16`, `'17`, 
         `'18`, `'19`, `'20`, `'21`, `'22`)

# print table
data_table_1 |>
  select(field, 
         Aggregate = median_median_n, 
         starts_with("'")) |>
         #starts_with("NF")) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

### Figure

```{r}

# Filter out non-social studies published in psych science
# calculate median Ns
# order field column by same used in Table 1
data_figure_1 <- data_table_1_filtered |>
  group_by(field, year) |>
  summarize(median_n = janitor::round_half_up(median(N, na.rm = TRUE), 0)) |>
  ungroup() |>
  mutate(field = fct_rev(fct_relevel(field, levels(data_table_1$field))))

# plot 
ggplot(data_figure_1, aes(as.factor(year), median_n, group = field)) +
  #geom_smooth(method = "lm") +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "plasma", begin = 0.1, end = 0.9) +
  theme_classic() +
  ylab("Median sample size (N) per study") +
  xlab("Year") 

```

# IRAP studies sample size by cell

## Change in N-pact over time 

Results represent Median N (k studies)

### Table

```{r}

# Filter out non-social studies published in psych science
data_table_2_filtered <- data_combined_using_nhst |>
  filter(journal == "IRAP" & design %in% c("between", "mixed")) |>
  mutate(N_per_cell = N/n_groups_between)

# calculate median N for each field and year
# round using typical half-up rule and not R's weird rounding
data_table_2_n_long <- data_table_2_filtered |>
  group_by(field, year) |>
  summarize(median_n_per_cell = janitor::round_half_up(median(N_per_cell, na.rm = TRUE), 0)) |>
  ungroup()

# Calculate median median n for each field
data_table_2_n_aggregate <- data_table_2_n_long |>
  group_by(field) |>
  summarize(median_median_n_per_cell = median(median_n_per_cell, na.rm = TRUE)) |>
  ungroup()

# Calculate k studies used to calculate median ns
data_table_2_k_long <- data_table_2_filtered |>
  count(field, year, name = "k_studies")

# combine Ns and Ks, reshape, and order field column by aggregate median median n 
data_table_2 <- 
  full_join(data_table_2_n_long, data_table_2_k_long, by = c("field", "year")) |>
  mutate(result = paste0(median_n_per_cell, " (", k_studies, ")")) |>
  select(-median_n_per_cell, -k_studies) |>
  pivot_wider(names_from = year, 
              values_from = result) |>
              #names_prefix = "NF ") |>
  left_join(data_table_2_n_aggregate, by = "field") |>
  mutate(field = fct_reorder(field, median_median_n_per_cell)) |>
  arrange(desc(field)) |>
  select(field, median_median_n_per_cell, 
         `'06`, `'07`, `'08`, `'09`, `'10`, `'11`, 
         `'12`, `'13`, `'14`, `'15`, `'16`, `'17`, 
         `'18`, `'19`, `'20`, `'21`, `'22`)

# print table
data_table_2 |>
  select(field, 
         Aggregate = median_median_n_per_cell, 
         starts_with("'")) |>
         #starts_with("NF")) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

### Figure

```{r}

# Filter out non-social studies published in psych science
# calculate median Ns
# order field column by same used in Table 1
data_figure_2 <- data_table_2_filtered |>
  group_by(field, year) |>
  summarize(median_n_per_cell = janitor::round_half_up(median(N_per_cell, na.rm = TRUE), 0)) |>
  ungroup() |>
  mutate(field = fct_rev(fct_relevel(field, levels(data_table_1$field))))

# plot 
ggplot(data_figure_2, aes(as.factor(year), median_n_per_cell, group = field)) +
  #geom_smooth(method = "lm") +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "plasma", begin = 0.1, end = 0.9) +
  theme_classic() +
  ylab("Median sample size (N) per cell") +
  xlab("Year") 

```

#### Combined plot

Probably most useful for publication

```{r}

# combine data
bind_rows(
  data_figure_1 |>
    mutate(type = "Participants per study"),
  data_figure_2 |>
    rename(median_n = median_n_per_cell) |>
    mutate(type = "Participants per cell")
) |>
ggplot(aes(as.factor(year), median_n, group = type, color = type)) +
  #geom_smooth(method = "lm") +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "viridis", begin = 0.3, end = 0.7,
                        guide = guide_legend(reverse = TRUE)) +
  theme_classic() +
  ylab("Median sample size (N)") +
  xlab("Year") +
  labs(colour = "")

```

## Estimated Statistical Power 

to Detect an Average Published Effect Size ($\rho$ = .20) over time

### Table

```{r}

# use median n to calculate power
data_table_3_long <- data_table_2_n_long |>
  #mutate(power = janitor::round_half_up(pwr.r.test(r = .2, sig.level = 0.05, n = median_n_per_cell)$power*100, 0)) |>
  mutate(power = janitor::round_half_up(pwr.t.test(d = .408, sig.level = 0.05, n = median_n_per_cell, type = "two.sample")$power*100, 0)) |>
  select(-median_n_per_cell)

# reshape table, order field column by that used in previous table
data_table_3 <- data_table_3_long |>
  pivot_wider(names_from = year, 
              values_from = power) |>
              #names_prefix = "power ") |>
  mutate(field = fct_relevel(field, levels(data_table_1$field))) |>
  arrange(desc(field)) |>
  select(field, 
         `'06`, `'07`, `'08`, `'09`, `'10`, `'11`, 
         `'12`, `'13`, `'14`, `'15`, `'16`, `'17`, 
         `'18`, `'19`, `'20`, `'21`, `'22`)

# print table
data_table_3 |>
  kable() |>
  kable_classic(full_width = FALSE)

```

### Figure

```{r}

# order field column by same used in Table 1, then plot
data_table_3_long |>
  mutate(field = fct_rev(fct_relevel(field, levels(data_table_1$field)))) |>
  ggplot(aes(as.factor(year), power/100, group = field)) +
  geom_hline(yintercept = .80, linetype = "dotted") +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "plasma", begin = 0.1, end = 0.9) +
  scale_y_continuous(labels = scales::label_comma(), 
                     breaks = c(0, .2, .4, .6, .8, 1), 
                     limits = c(0,1)) +
  theme_classic() +
  ylab("Power") +
  xlab("Year")

```

## Estimated False-Discovery Rates 

### Table

Assuming No p-Hacking or Questionable Research Practices

Using various assumptions about the P(null = TRUE)

```{r}

# select a subset power data from first and last year studied, reshape 
data_power_subset <- data_table_3 |>
  select(field, `'11`, `'19`) |>
  pivot_longer(cols = c(`'11`, `'19`), 
               names_to = "year",
               values_to = "power") 

# duplicate data with different values of $P(H_0)$,
# calculate False Discovery Rate
data_table_4 <- bind_rows(
  mutate(data_power_subset, baserate_null = .50),
  mutate(data_power_subset, baserate_null = .80)
) |>
  mutate(power = power/100,
         fdr = janitor::round_half_up( (baserate_null*.05) / (  (baserate_null*.05) + ((1-baserate_null)*power)  ), 2)) |>
  select(field, year, baserate_null, fdr) |>
  pivot_wider(names_from = c(year, baserate_null),
              values_from = fdr) |>
  select(field, `'11_0.5`, `'11_0.8`, `'19_0.5`, `'19_0.8`)

# print table
data_table_4 |>
  kable(col.names = c("field", "$P(H_0)$ = 0.5", "$P(H_0)$ = 0.8", "$P(H_0)$ = 0.5", "$P(H_0)$ = 0.8"),
        escape = FALSE) |>
  kable_classic(full_width = FALSE) %>%
  add_header_above(c(" " = 1, "2011" = 2, "2019" = 2))

```

# IRAP vs Social Psychology studies using study N (after exclusions)

More assumptions by using sample size N rather than N per cell. 

## Change in N-pact over time 

Results represent Median N (k studies)

### Table

```{r}

# Filter out non-social studies published in psych science
data_table_5_filtered <- data_combined |>
  filter(
    (design == "between" &
      ((journal == "PS:S" & (social_ps == 1 | is.na(social_ps))) | journal != "PS:S")) |
      (journal == "IRAP" & design %in% c("between", "mixed") & used_inferential_statistics == TRUE & reported_n == TRUE)
  )

# calculate median N for each field and year
# round using typical half-up rule and not R's weird rounding
data_table_5_n_long <- data_table_5_filtered |>
  group_by(field, year) |>
  summarize(median_n = janitor::round_half_up(median(N, na.rm = TRUE), 0)) |>
  ungroup()

# Calculate median median n for each field
data_table_5_n_aggregate <- data_table_5_n_long |>
  group_by(field) |>
  summarize(median_median_n = median(median_n, na.rm = TRUE)) |>
  ungroup()

# Calculate k studies used to calculate median ns
data_table_5_k_long <- data_table_5_filtered |>
  count(field, year, name = "k_studies")

# combine Ns and Ks, reshape, and order field column by aggregate median median n 
data_table_5 <- 
  full_join(data_table_5_n_long, data_table_5_k_long, by = c("field", "year")) |>
  mutate(result = paste0(median_n, " (", k_studies, ")")) |>
  select(-median_n, -k_studies) |>
  pivot_wider(names_from = year, 
              values_from = result) |>
              #names_prefix = "NF ") |>
  left_join(data_table_5_n_aggregate, by = "field") |>
  mutate(field = fct_reorder(field, median_median_n)) |>
  arrange(desc(field)) |>
  select(field, median_median_n, 
         `'06`, `'07`, `'08`, `'09`, `'10`, `'11`, 
         `'12`, `'13`, `'14`, `'15`, `'16`, `'17`, 
         `'18`, `'19`, `'20`, `'21`, `'22`)

# print table
data_table_5 |>
  select(field, 
         Aggregate = median_median_n, 
         starts_with("'")) |>
         #starts_with("NF")) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

### Figure

```{r}

# Filter out non-social studies published in psych science
# calculate median Ns
# order field column by same used in Table 1
data_figure_4 <- data_table_5_filtered |>
  group_by(field, year) |>
  summarize(median_n = janitor::round_half_up(median(N, na.rm = TRUE), 0)) |>
  ungroup() |>
  mutate(field = fct_rev(fct_relevel(field, levels(data_table_1$field))))

# plot 
ggplot(data_figure_4, aes(as.factor(year), median_n, color = field, group = field)) +
  #geom_vline(xintercept = "2011", linetype = "dotted") +
  #geom_smooth(method = "lm", alpha = 0.2) +
  geom_line() +
  geom_point() +
  scale_y_continuous(breaks = c(0, 50, 100, 150, 200, 250, 300)) +
  scale_color_viridis_d(option = "viridis", begin = 0.3, end = 0.7) +
  theme_classic() +
  ylab("Median sample size (N) per study") +
  xlab("Year") +
  labs(colour = "Field")

```

## Estimated statistical power 

to Detect an Average Published Effect Size ($\rho$ = .20) over time

### Table

```{r}

# use median n to calculate power
data_table_6_long <- data_table_5_n_long |>
  mutate(power = janitor::round_half_up(pwr.r.test(r = .2, sig.level = 0.05, n = median_n)$power*100, 0)) |>
  select(-median_n)

# reshape table, order field column by that used in previous table
data_table_6 <- data_table_6_long |>
  pivot_wider(names_from = year, 
              values_from = power) |>
              #names_prefix = "power ") |>
  mutate(field = fct_relevel(field, levels(data_table_1$field))) |>
  arrange(desc(field)) |>
  select(field, 
         `'06`, `'07`, `'08`, `'09`, `'10`, `'11`, 
         `'12`, `'13`, `'14`, `'15`, `'16`, `'17`, 
         `'18`, `'19`, `'20`, `'21`, `'22`)

# print table
data_table_6 |>
  kable() |>
  kable_classic(full_width = FALSE)

```

### Figure

```{r}

# order field column by same used in Table 1, then plot
data_table_6_long |>
  mutate(field = fct_rev(fct_relevel(field, levels(data_table_1$field)))) |>
  ggplot(aes(as.factor(year), power/100, color = field, group = field)) +
  geom_hline(yintercept = .80, linetype = "dotted") +
  geom_line() +
  geom_point() +
  scale_color_viridis_d(option = "viridis", begin = 0.3, end = 0.7) +
  theme_classic() +
  ylab("Power") +
  xlab("Year") +
  labs(colour = "field") +
  scale_y_continuous(labels = scales::label_comma(), 
                     breaks = c(0, .2, .4, .6, .8, 1), 
                     limits = c(0,1)) +
  labs(colour = "Field")

```

## Estimated False-Discovery Rates 

Assuming No p-Hacking or Questionable Research Practices

Using various assumptions about the P(null = TRUE)

### Table

```{r}

# select a subset power data from first and last year studied, reshape 
data_power_subset <- data_table_6 |>
  select(field, `'11`, `'19`) |>
  pivot_longer(cols = c(`'11`, `'19`), 
               names_to = "year",
               values_to = "power") 

# duplicate data with different values of $P(H_0)$,
# calculate False Discovery Rate
data_table_7 <- bind_rows(
  mutate(data_power_subset, baserate_null = .50),
  mutate(data_power_subset, baserate_null = .80)
) |>
  mutate(power = power/100,
         fdr = janitor::round_half_up( (baserate_null*.05) / (  (baserate_null*.05) + ((1-baserate_null)*power)  ), 2)) |>
  select(field, year, baserate_null, fdr) |>
  pivot_wider(names_from = c(year, baserate_null),
              values_from = fdr) |>
  select(field, `'11_0.5`, `'11_0.8`, `'19_0.5`, `'19_0.8`)

# print table
data_table_7 |>
  kable(col.names = c("field", "$P(H_0)$ = 0.5", "$P(H_0)$ = 0.8", "$P(H_0)$ = 0.5", "$P(H_0)$ = 0.8"),
        escape = FALSE) |>
  kable_classic(full_width = FALSE) %>%
  add_header_above(c(" " = 1, "2011" = 2, "2019" = 2))

```


